{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox61Jknz6Dte"
      },
      "outputs": [],
      "source": [
        "!pip install keras\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install tensorflow\n",
        "!pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9yAlfVL0jne"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import PIL\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "import os\n",
        "from keras.models import Model\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.layers import Dense, Flatten\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import lime\n",
        "from lime import lime_image\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JdR9t78i1ydE"
      },
      "outputs": [],
      "source": [
        "!wget \"http://seppe.net/aa/assignment2/images.zip\"\n",
        "!7z x images.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_HtYuKC32dc",
        "outputId": "3dd76deb-aab1-400b-9665-b8221c03d29f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3yf9caQ5FJJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "images_path = '/content/images.zip'\n",
        "dataset_path = '/content/gdrive/My Drive/dataset.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyMYuVPC5vnv"
      },
      "source": [
        "Extract the price categories and store them in a library where they are linked to images. (We will later need this as a dataframe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StzBljSJ5fwq"
      },
      "outputs": [],
      "source": [
        "with open(dataset_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "price_categories = {}\n",
        "image_ids = []\n",
        "for item in data:\n",
        "    # Check if 'price_category' key is present in the 'item' dictionary\n",
        "    if 'price_category' in item and item['price_category'] is not None:\n",
        "        # Extract the price category\n",
        "        price_category = item['price_category']['label']\n",
        "        # Extract the image IDs from the \"full_images\" list and map them to their price category\n",
        "        for img in item[\"more_details\"][\"full_images\"]:\n",
        "            image_id = img[\"image_id\"]\n",
        "            image_ids.append(image_id)\n",
        "            price_categories[image_id] = price_category\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf4B92iD6RoH"
      },
      "source": [
        "Now I have a list of image id's and a library of price categories.\n",
        "The file names of the images are just the id with .jpg behind it.\n",
        "I want to create the training, validation and test set.\n",
        "For this I need to make a pandas data frame with two columns, with the image file names and one with the categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpcmtlhi62LX"
      },
      "source": [
        "First, we shuffle the image id's and then split them up in training, validation and test set lists of image id's.(The training, validation, test sizes are 70%, 20% and 10%. We could, however **change these values 0.7, 0.2, 0.1 to lower values if we want to use less of the data**.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC_6BcQK6eDk"
      },
      "outputs": [],
      "source": [
        "random.shuffle(image_ids)  # shuffle image id's\n",
        "\n",
        "train_size = 0.07\n",
        "val_size = 0.02\n",
        "test_size = 0.01\n",
        "\n",
        "train_image_ids = image_ids[:int(len(image_ids) * train_size)]\n",
        "val_image_ids = image_ids[int(len(image_ids) * train_size):int(len(image_ids) * (train_size + val_size))]\n",
        "test_image_ids = image_ids[int(len(image_ids) * (train_size + val_size)):]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqDmaBMo7A-h"
      },
      "source": [
        "Now we create lists of the filenames of these images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKlFQ-Nu6tCS"
      },
      "outputs": [],
      "source": [
        "train_filenames = [f\"{image_id}.jpg\" for image_id in train_image_ids]\n",
        "val_filenames = [f\"{image_id}.jpg\" for image_id in val_image_ids]\n",
        "test_filenames = [f\"{image_id}.jpg\" for image_id in test_image_ids]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKSWOfHO7NdG"
      },
      "source": [
        "Now, we create a pandas dataframe with two columns: \"filename\" and \"price_category\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG3noLTe7QZG"
      },
      "outputs": [],
      "source": [
        "train_df = pd.DataFrame(\n",
        "    {\"filename\": train_filenames, \"price_category\": [price_categories[image_id] for image_id in train_image_ids]})\n",
        "val_df = pd.DataFrame(\n",
        "    {\"filename\": val_filenames, \"price_category\": [price_categories[image_id] for image_id in val_image_ids]})\n",
        "test_df = pd.DataFrame(\n",
        "    {\"filename\": test_filenames, \"price_category\": [price_categories[image_id] for image_id in test_image_ids]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rZ1tjSR7iJ7"
      },
      "source": [
        "We set the paths for the training, validation, and test data directories and create the directories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PNxNMKa7yC8"
      },
      "source": [
        "We set the paths for the training, validation, and test data directories and create the directories."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the paths for the training, validation, and test data directories\n",
        "train_data_dir = os.path.join(images_path, \"train\")\n",
        "val_data_dir = os.path.join(images_path, \"val\")\n",
        "test_data_dir = os.path.join(images_path, \"test\")\n",
        "\n",
        "# Create the directories if they don't exist already\n",
        "!mkdir -p \"{train_data_dir}\"\n",
        "!mkdir -p \"{val_data_dir}\"\n",
        "!mkdir -p \"{test_data_dir}\""
      ],
      "metadata": {
        "id": "743xyvxt6zr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-mbG6FJ7m2_"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = '/content/images.zip'\n",
        "target_dir = '/content/images'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(target_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5yxY8iL70Qx"
      },
      "source": [
        "We copy the image files to the appropriate directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1Vsr91l74LQ"
      },
      "outputs": [],
      "source": [
        "for filename in train_filenames:\n",
        "    src_path = os.path.join(target_dir, filename)\n",
        "    dst_path = os.path.join(train_data_dir, filename)\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "for filename in val_filenames:\n",
        "    src_path = os.path.join(target_dir, filename)\n",
        "    dst_path = os.path.join(val_data_dir, filename)\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "for filename in test_filenames:\n",
        "    src_path = os.path.join(target_dir, filename)\n",
        "    dst_path = os.path.join(test_data_dir, filename)\n",
        "    shutil.copy(src_path, dst_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qxGLyur78AJ"
      },
      "source": [
        "Now we want to preprocess the data and then generate it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU7I-sIA8ET2"
      },
      "source": [
        "We first define the image dimensions and the batch size. **(Change this, see what works best)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnN2pDNm8OzT"
      },
      "outputs": [],
      "source": [
        "img_width, img_height = 224, 224\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76zB8ctT8Yft"
      },
      "source": [
        "We use ImageDataGenerator for preprocessing including rotation augmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pcP1WNg8hEn"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1. / 255, rotation_range=20, fill_mode='nearest')\n",
        "val_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH8dsZSY85E-"
      },
      "source": [
        "We generate the training, validation, and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0pUduEu89lz",
        "outputId": "d5a21916-3e09-4f8b-de49-023a8d7f6b2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 105071 images belonging to 1 classes.\n",
            "Found 105071 images belonging to 1 classes.\n",
            "Found 105071 images belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "        '/content/images/train',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "        '/content/images/val',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')\n",
        "\n",
        "test_generator = datagen.flow_from_directory(\n",
        "        '/content/images/test',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIh45Yfp9AbW"
      },
      "source": [
        "Now we can finally build our Neural Network. We created one by hand at first, and then used a pretrained model (VGG16) to compare the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnMaw7ar9ao1"
      },
      "source": [
        "Define the CNN structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49pjF2_z9Yaw"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(img_width, img_height, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(4, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9uCOxmr9dxj"
      },
      "source": [
        "Compile the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8CuxvPz9h0O"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_PtQqly9ikU"
      },
      "source": [
        "Train the model.(**Number of epochs can be changed**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0ZgNxh09m5d"
      },
      "outputs": [],
      "source": [
        "history = model.fit_generator(train_generator, steps_per_epoch=train_generator.samples // batch_size, epochs=50,\n",
        "                              validation_data=val_generator, validation_steps=val_generator.samples // batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYf9TBa9-M8m"
      },
      "source": [
        "Evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9ueSghU-Q2K"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate_generator(test_generator, steps=test_generator.samples // batch_size)\n",
        "print('Test accuracy:', test_acc)\n",
        "print('Test loss:', test_loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzek8u6o-R0x"
      },
      "source": [
        "Interpreter LIME\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UTs_J3r-UCh"
      },
      "outputs": [],
      "source": [
        "explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "\n",
        "def predict_wrapper(images):\n",
        "    # This function is a wrapper around the model's prediction function\n",
        "    # It takes in a batch of images (N, 224, 224, 3) and returns the predicted probabilities (N, 5)\n",
        "    return model.predict(images)\n",
        "\n",
        "\n",
        "# Get an example image from the validation set\n",
        "example_image_path = os.path.join(val_data_dir, val_filenames[0])\n",
        "example_image = Image.open(example_image_path)\n",
        "\n",
        "# Explain the model's prediction for the example image\n",
        "explanation = explainer.explain_instance(np.array(example_image), predict_wrapper, top_labels=5)\n",
        "\n",
        "# Show the LIME visualization for the explanation\n",
        "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=True)\n",
        "img_boundry = mark_boundaries(temp/2 + 0.5, mask)\n",
        "Image.fromarray((img_boundry*255).astype(np.uint8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9P21CzA-bgD"
      },
      "source": [
        "Now, we do the same thing but with the pretrained model VGG16."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv5sMW7li9ee"
      },
      "source": [
        "We first preprocess and generate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "3Oe7g_gBjEwm",
        "outputId": "69aaba03-8729-40ff-8b77-0ad22bd460b6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-93d0d3e0333c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                                      \u001b[0mvertical_flip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                      \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                      preprocessing_function=preprocess_input)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mval_datagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreprocessing_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtest_datagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreprocessing_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocess_input' is not defined"
          ]
        }
      ],
      "source": [
        "# preprocessing according to VGG16\n",
        "batch_size = 64\n",
        "train_datagen = ImageDataGenerator(rescale=1. / 255, rotation_range=90,\n",
        "                                     brightness_range=[0.1, 0.7],\n",
        "                                     width_shift_range=0.5,\n",
        "                                     height_shift_range=0.5,\n",
        "                                     horizontal_flip=True,\n",
        "                                     vertical_flip=True,\n",
        "                                     validation_split=0.15,\n",
        "                                     preprocessing_function=preprocess_input)\n",
        "val_datagen = ImageDataGenerator(rescale=1. / 255,preprocessing_function=preprocess_input)\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255,preprocessing_function=preprocess_input)\n",
        "\n",
        "# Generate the training, validation, and testing datasets (I followed the instructions on https://www.learndatasci.com/tutorials/hands-on-transfer-learning-keras/)\n",
        "train_generator = train_datagen.flow_from_directory(train_data_dir, target_size=(img_width, img_height),\n",
        "                                                    batch_size=batch_size, class_mode='categorical', shuffle=True,\n",
        "                                               seed=42)\n",
        "val_generator = val_datagen.flow_from_directory(val_data_dir, target_size=(img_width, img_height),\n",
        "                                                batch_size=batch_size, class_mode='categorical', shuffle=True,\n",
        "                                               seed=42)\n",
        "test_generator = test_datagen.flow_from_directory(test_data_dir,\n",
        "                                             target_size=(img_width, img_height),\n",
        "                                             class_mode='categorical',\n",
        "                                             batch_size=1,\n",
        "                                             shuffle=False,\n",
        "                                             seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y2452ok-0C-"
      },
      "source": [
        "Load without the top layer input_shape(img_width, img_height, 3) (the 3 is for the colors rbg)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDR-mYWu-xK9"
      },
      "outputs": [],
      "source": [
        "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "for layer in vgg_model.layers:\n",
        "    layer.trainable = False\n",
        "    # This is to make sure we don't train these layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReBGx9JP_E8h"
      },
      "source": [
        "We add a new output layer with 4 categories. Then we create the new model with the pretrained layers and our new output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3DTqsuN_FKR"
      },
      "outputs": [],
      "source": [
        "x = Flatten()(vgg_model.output)\n",
        "output_layer = Dense(4, activation='softmax')(x)\n",
        "\n",
        "model_VGG = Model(inputs=vgg_model.input, outputs=output_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0pHvnq7_SAu"
      },
      "source": [
        "Compile the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyU12H0q_VCB"
      },
      "outputs": [],
      "source": [
        "model_VGG.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pyVb_Iu_VMd"
      },
      "source": [
        "Train the model. (**Number of epochs can be changed**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiCLdcKS_W1e"
      },
      "outputs": [],
      "source": [
        "history = model_VGG.fit_generator(train_generator, steps_per_epoch=train_generator.samples // batch_size, epochs=5,\n",
        "                              validation_data=val_generator, validation_steps=val_generator.samples // batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lBbD7rO_q4a"
      },
      "source": [
        "Evaluate the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTqXorN7_s2C"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model_VGG.evaluate_generator(test_generator, steps=test_generator.samples // batch_size)\n",
        "print('Test accuracy:', test_acc)\n",
        "print('Test loss:', test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpCyg39q_tvx"
      },
      "source": [
        "LIME interpreter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PR3cDmIW_vR6"
      },
      "outputs": [],
      "source": [
        "explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "\n",
        "def predict_wrapper_VGG(images):\n",
        "    # This function is a wrapper around the model's prediction function\n",
        "    # It takes in a batch of images (N, 224, 224, 3) and returns the predicted probabilities (N, 5)\n",
        "    return model_VGG.predict(images)\n",
        "\n",
        "\n",
        "# Get an example image from the validation set\n",
        "example_image_path = os.path.join(val_data_dir, val_filenames[0])\n",
        "example_image = Image.open(example_image_path)\n",
        "\n",
        "# Explain the model's prediction for the example image\n",
        "explanation = explainer.explain_instance(np.array(example_image), predict_wrapper_VGG, top_labels=5)\n",
        "\n",
        "# Show the LIME visualization for the explanation\n",
        "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=True)\n",
        "img_boundry = mark_boundaries(temp/2 + 0.5, mask)\n",
        "Image.fromarray((img_boundry*255).astype(np.uint8))"
      ]
    }
  ]
}